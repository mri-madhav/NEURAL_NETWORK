
---

## ğŸ”¢ Mathematical Details

### Activation Functions
- ReLU:  f(z) = max(0, z)
- Softmax: Converts logits into class probabilities

### Loss Function
Categorical Cross-Entropy:
L = - (1 / m) âˆ‘ y log(Å·)

Using Softmax + Cross-Entropy gives the simplified gradient:
âˆ‚L / âˆ‚Z = Å· âˆ’ y

---

## âš™ï¸ Implementation Details
- Manual implementation of forward propagation and backpropagation
- He initialization for stable training with ReLU
- Numerically stable softmax and loss
- Vectorized NumPy operations
- Reproducible results using fixed random seed

---

## ğŸ“Š Training Setup

| Parameter | Value |
|----------|-------|
| Optimizer | Gradient Descent |
| Learning Rate | 0.01 |
| Epochs | 1000 |
| Initialization | He Initialization |
| Batch Type | Full Batch |

---

## âœ… Results

| Dataset | Accuracy |
|--------|----------|
| Training | ~97â€“98% |
| Test | ~95â€“96% |

---

## ğŸ“ Project Structure
